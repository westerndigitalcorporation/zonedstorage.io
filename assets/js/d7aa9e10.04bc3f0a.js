"use strict";(self.webpackChunkzonedstorage_io=self.webpackChunkzonedstorage_io||[]).push([[1519],{5358:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"filesystems/xfs","title":"XFS","description":"XFS is a widely used scalable enterprise file system that traces it\'s root","source":"@site/docs/filesystems/xfs.md","sourceDirName":"filesystems","slug":"/filesystems/xfs","permalink":"/docs/filesystems/xfs","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"xfs","title":"XFS","sidebar_label":"XFS"},"sidebar":"docs","previous":{"title":"Overview","permalink":"/docs/filesystems"},"next":{"title":"BTRFS","permalink":"/docs/filesystems/btrfs"}}');var i=t(4848),a=t(8453);const o={id:"xfs",title:"XFS",sidebar_label:"XFS"},r="XFS File System",l={},d=[{value:"Usage",id:"usage",level:2},{value:"Zoned Block Device with Conventional Zones",id:"zoned-block-device-with-conventional-zones",level:3},{value:"Other Zoned Block Devices",id:"other-zoned-block-devices",level:3},{value:"Tuning",id:"tuning",level:3},{value:"Placing Data on the Main Device",id:"placing-data-on-the-main-device",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Limitations",id:"limitations",level:2},{value:"System Requirements",id:"system-requirements",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"xfs-file-system",children:"XFS File System"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," is a widely used scalable enterprise file system that traces it's root\nback to the IRIX operating system in the 1990s and is supported by all\nmajor Linux distributions. ",(0,i.jsx)(n.em,{children:"XFS"})," was originally designed for in-place overwrites\non conventional storage devices, but has experimental support for zoned block\ndevices."]}),"\n",(0,i.jsx)(n.admonition,{title:"System Requirements",type:"note",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Linux kernel: 6.15+ with CONFIG_XFS_FS and CONFIG_XFS_RT enabled"}),"\n",(0,i.jsx)(n.li,{children:"xfsprogs: 6.15+"}),"\n"]})}),"\n",(0,i.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,i.jsxs)(n.p,{children:["First, check that your system meets the\n",(0,i.jsx)(n.a,{href:"/docs/filesystems/xfs#system-requirements",children:"requirements"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Formatting a zoned block device with ",(0,i.jsx)(n.em,{children:"XFS"})," requires a separate randomly\nwriteable block device for metadata, unless the device has conventional zones\nat the start of its address space."]}),"\n",(0,i.jsx)(n.h3,{id:"zoned-block-device-with-conventional-zones",children:"Zoned Block Device with Conventional Zones"}),"\n",(0,i.jsxs)(n.p,{children:["Formatting a zoned block device that has at least about 1% of its capacity\nreported as conventional zones at the beginning of its address space does not\nrequire a separate device for ",(0,i.jsx)(n.em,{children:"XFS"})," metadata. This is the case for most SMR\nhard-disks. For such device, ",(0,i.jsx)(n.em,{children:"XFS"})," automatically and transparently splits the\ndevice into different logical address spaces representing the main and realtime\ndevices. The main device is composed of the device conventional zones and\nused to store ",(0,i.jsx)(n.em,{children:"XFS"})," metadata. The realtime device is composed of the zoned\ndevice sequential write required zones and used by default to store file data."]}),"\n",(0,i.jsxs)(n.p,{children:["The following example formats an SMR hard-disk for use with ",(0,i.jsx)(n.em,{children:"XFS"})," using the\ncommand ",(0,i.jsx)(n.code,{children:"mkfs.xfs"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# mkfs.xfs -f /dev/sda\nmeta-data=/dev/sda               isize=512    agcount=4, agsize=18317312 blks\n         =                       sectsz=4096  attr=2, projid32bit=1\n         =                       crc=1        finobt=1, sparse=1, rmapbt=1\n         =                       reflink=0    bigtime=1 inobtcount=1 nrext64=1\n         =                       exchange=1   metadir=1\ndata     =                       bsize=4096   blocks=73269248, imaxpct=25\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1, parent=0\nlog      =internal log           bsize=4096   blocks=35776, version=2\n         =                       sectsz=4096  sunit=1 blks, lazy-count=1\nrealtime =internal               extsz=4096   blocks=7251034112, rtextents=7251034112\n         =                       rgcount=110642 rgsize=65536 extents\n         =                       zoned=1      start=73269248 reserved=0\nResetting zones...Done.\n"})}),"\n",(0,i.jsx)(n.p,{children:"The formatted disk can now be mounted. No other setup is necessary."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# mount /dev/sda /mnt\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The kernel messages will signal that ",(0,i.jsx)(n.em,{children:"XFS"})," zoned device support is still\nlabelled as ",(0,i.jsx)(n.em,{children:"experimental"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"XFS (sda): EXPERIMENTAL zoned RT device feature enabled.  Use at your own risk!\nXFS (sda): Mounting V5 Filesystem d2d4bc64-7bcb-4b25-bb05-22352a4ab4b4\nXFS (sda): Ending clean mount\nXFS (sda): 110642 zones of 65536 blocks size (128 max open)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"other-zoned-block-devices",children:"Other Zoned Block Devices"}),"\n",(0,i.jsxs)(n.p,{children:["To format zoned block devices lacking conventional zones for use with ",(0,i.jsx)(n.em,{children:"XFS"}),"\n(e.g. an NVMe Zoned Namespace SSD), a separate conventional block device must\nbe prepared to use as ",(0,i.jsx)(n.em,{children:"XFS"})," main device (for storing metadata). Typically, the\nmain device capacity should be at least 1% of the total capacity of the zoned\nblock device. If the zoned block device used is an NVMe ZNS SSD, this separate\nconventional block device can be a separate conventional namespace on the same\nSSD.  Zoned block devices that have conventional zones can also be used only\nfor the realtime device. This is useful to combine an SSD for metadata with an\nHDD for file data."]}),"\n",(0,i.jsx)(n.p,{children:"In this example, the (zoned) NVMe ZNS SSD /dev/nvme0n2 is used as the realtime\ndevice and the conventional SSD /dev/nvme2n1 is used as the main device."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# mkfs.xfs -r rtdev=/dev/nvme0n2 /dev/nvme2n1\nmeta-data=/dev/nvme2n1           isize=512    agcount=16, agsize=61047165 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n         =                       crc=1        finobt=1, sparse=1, rmapbt=1\n         =                       reflink=0    bigtime=1 inobtcount=1 nrext64=1\n         =                       exchange=1   metadir=1\ndata     =                       bsize=4096   blocks=976754640, imaxpct=5\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1, parent=0\nlog      =internal log           bsize=4096   blocks=476930, version=2\n         =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =/dev/nvme0n2           extsz=4096   blocks=999180288, rtextents=999180288\n         =                       rgcount=3624 rgsize=275712 extents\n         =                       zoned=1      start=0 reserved=0\nDiscarding blocks...Done.\nResetting zones...Done.\n"})}),"\n",(0,i.jsx)(n.p,{children:"Mounting the file system requires specifying both the main and realtime devices\nwith the mount command."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# mount -o rtdev=/dev/nvme0n2 /dev/nvme2n1 /mnt\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The kernel messages will signal that ",(0,i.jsx)(n.em,{children:"XFS"})," zoned device support is still\nlabelled as ",(0,i.jsx)(n.em,{children:"experimental"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"XFS (nvme2n1): EXPERIMENTAL zoned RT device feature enabled.  Use at your own risk!\nXFS (nvme2n1): Mounting V5 Filesystem eb1fe112-bb66-4fec-9e6d-24cc2dfeb48e\nXFS (nvme2n1): Ending clean mount\nXFS (nvme2n1): 3624 zones of 275712 blocks size (14 max open)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"tuning",children:"Tuning"}),"\n",(0,i.jsxs)(n.p,{children:["By default, ",(0,i.jsx)(n.em,{children:"XFS"})," uses the maximum number of open zones of the device minus one\nas the limit for the maximum number of zones that can be simultaneously used to\nwrite file data. For a mounted file system, this limit can be checked using the\n",(0,i.jsx)(n.em,{children:"max_open_zones"})," configfs attribute."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# cat /sys/block/sda/queue/max_open_zones\n128\n# mkfs.xfs /dev/sda\n# mount /dev/sda /mnt\n# cat /sys/fs/xfs/sda/zoned/max_open_zones\n127\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Depending on the application workload and the zoned device used, reducing this\nlimit can improve write performance, albeit at the risk of increasing file\nfragmentation, which can negatively impact read performance. Reducing the limit\non the maximum number of open realtime groups is done using the ",(0,i.jsx)(n.em,{children:"max_open_zones"}),"\nmount option."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# mount -o max_open_zones=32 /dev/sdb /mnt\n# cat /sys/fs/xfs/sdb/zoned/max_open_zones\n31\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The garbage collection threshold threshold can be configured in ",(0,i.jsx)(n.em,{children:"sysfs"})," for\neach file system using the ",(0,i.jsx)(n.em,{children:"zonegc_low_space"})," attribute."]}),"\n",(0,i.jsxs)(n.p,{children:["The default attribute value is 0, indicating that ",(0,i.jsx)(n.em,{children:"XFS"})," tries to ensures that\nthere is at least one free zone available for each open zone, as long as there\nis free space available."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# mount /dev/sdb /mnt\n# cat /sys/fs/xfs/sdb/zoned/zonegc_low_space\n0\n"})}),"\n",(0,i.jsxs)(n.p,{children:["To more aggressively garbage-collect reclaimable zones, ",(0,i.jsx)(n.em,{children:"zonegc_low_space"})," can\nbe increased. A value X indicates that ",(0,i.jsx)(n.em,{children:"XFS"})," will attempt reclaiming zones to\nhave X percent of the unused storage space available for writing."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# echo 5 > /sys/fs/xfs/sdb/zoned/zonegc_low_space\n# cat /sys/fs/xfs/sdb/zoned/zonegc_low_space\n5\n"})}),"\n",(0,i.jsx)(n.h3,{id:"placing-data-on-the-main-device",children:"Placing Data on the Main Device"}),"\n",(0,i.jsxs)(n.p,{children:["In ",(0,i.jsx)(n.em,{children:"XFS"}),", the ",(0,i.jsx)(n.em,{children:"realtime"})," attribute for files controls if they are placed on the\nrealtime device, and the ",(0,i.jsx)(n.em,{children:"rtinherit"})," bit controls if files created inside a\ndirectory and subdirectories of it are placed on the realtime device."]}),"\n",(0,i.jsxs)(n.p,{children:["When using ",(0,i.jsx)(n.em,{children:"XFS"})," on a zoned block devices, all file data is placed by default\non the realtime (zoned) device. That is, the ",(0,i.jsx)(n.em,{children:"rtinherit"})," attribute on the file\nsystem root directory is set by default. The ",(0,i.jsx)(n.em,{children:"rtinherit"})," attribute can be\ncleared for any directory and new files created in that hierarchy will be placed\non the main device. Similarly the ",(0,i.jsx)(n.em,{children:"realtime"})," attribute can be cleared on a file\nthat does not have data allocated yet, and data allocated after that point will\nbe allocated on the main device."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# mkdir /mnt/dir\n# xfs_io -c 'lsattr' /mnt/dir\n-------t--------X /mnt/dir\n"})}),"\n",(0,i.jsxs)(n.p,{children:["A user can clear the ",(0,i.jsx)(n.em,{children:"rtinherit"})," attribute for a file or a directory to store\na file data on the main device."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# xfs_io -c 'chattr -t' /mnt/dir\n# xfs_io -c 'lsattr' /mnt/dir\n----------------X /mnt/dir\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:'Typically space on the main device is very limited and intended for metadata.\nSo this option should be used with care to avoid filling up the main device\ncapacity as that would result in "no space available" (ENOSPC) errors even if\nfree space is available on the realtime device.'})}),"\n",(0,i.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," zoned storage support is based on the ",(0,i.jsx)(n.em,{children:"XFS"})," realtime device feature.\nAn ",(0,i.jsx)(n.em,{children:"XFS"})," realtime device is a separate storage device managed by a separate\nspace allocator to store selective file data. ",(0,i.jsx)(n.em,{children:"XFS"})," uses the realtime device to\nmanage zoned storage by replacing the realtime space allocator with a zoned\nspace allocator. With zoned devices, by default file data is stored on\nthe realtime device, while metadata is stored on the main device, which must\nbe randomly writeable."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," metadata is space efficient thanks to the use of variable-length extents,\nand thus ",(0,i.jsx)(n.em,{children:"XFS"})," needs less than 1% conventional space for metadata. ",(0,i.jsx)(n.em,{children:"XFS"})," uses\nthe no-overwrite support originally added to support reference-counted block\nsharing (reflink) to support overwriting file data."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," splits the storage address space into ",(0,i.jsx)(n.em,{children:"allocation groups"})," for the main\ndevice and ",(0,i.jsx)(n.em,{children:"realtime groups"})," for the realtime device (the zoned block device).\n",(0,i.jsx)(n.em,{children:"XFS"})," manages free space and the physical-to-logical reverse mapping locally in\neach group to improve scalability.  With zoned block devices, each realtime\ngroup always maps to exactly one hardware zone on the device."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," tracks available free space using the persistent zone condition and zone\nwrite pointer provided by the zoned device, and thus does not need any\npersistent allocator metadata for the zoned realtime device."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," always writes file data on zoned block devices using Zone Append\noperations as that avoids serializing writes to a single zone.  For devices that\ndo not support Zone Append natively (e.g. SMR hard-disks), ",(0,i.jsx)(n.em,{children:"XFS"})," relies on the\nLinux block layer emulation (see ",(0,i.jsx)(n.a,{href:"/docs/linux/sched#zone-write-plugging",children:"Zone Write\nPlugging"}),")."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," can write to as many open groups (zones) as the hardware offers."]}),"\n",(0,i.jsx)(n.p,{children:"One open zone is always reserved for garbage collection, while all other open\nzones are available for user file I/Os. To ensure that the number of open zones\nstays under the hardware open zone limit, a new zone is only opened when all\nwrites to the previously open zone have completed."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," zone allocator by default tries to avoid fragmenting files. If the files\nbeing written to are still open, ",(0,i.jsx)(n.em,{children:"XFS"})," attempts to separate the data for each\nfile into different zones. When starting to write to a file, a free zone is\nchosen as the file data write location. If no free zone is available, or if the\nnumber of open zones has reached the limit, the least recently used open zone is\nused to write the file data. If the file has already been closed by the time of\nwriteback (buffered I/O case), file data are co-located to pack multiple files\ntightly to improve read performance."]}),"\n",(0,i.jsxs)(n.p,{children:["To reduce garbage collection overhead, the zone allocator aims to fill zones\nwith data of similar lifetime. The allocator assumes that all data in a file\nhave similar lifetime. The per-file ",(0,i.jsx)(n.em,{children:"F_SET_RW_HINT"})," file control system call\n(",(0,i.jsx)(n.em,{children:"fcntl"}),") interface allows applications to indicate if data is short, medium,\nlong, or extremely long lived to the kernel. ",(0,i.jsx)(n.em,{children:"XFS"})," uses these lifetime hints to\nguide zone selection: file data from different files marked as having the same\nlifetime are co-located in zones marked with the same lifetime."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," always immediately resets zones that do not contain valid blocks, but\notherwise does not perform background garbage collection. Instead, the garbage\ncollection thread (",(0,i.jsx)(n.em,{children:"GCD"}),") is woken up when the number of free zones drops below\nthe configurable threshold. Writers that get ahead of ",(0,i.jsx)(n.em,{children:"GCD"})," are put to sleep\nuntil ",(0,i.jsx)(n.em,{children:"GCD"})," recovers enough free zones."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"GCD"})," implements a greedy garbage collection policy and selects the reclaimable\nzone with the least number of valid blocks. ",(0,i.jsx)(n.em,{children:"GCD"})," does not hold locks over I/O,\nand thus garbage collection does not directly interfere with applications\naccessing files that ",(0,i.jsx)(n.em,{children:"GCD"})," operates on."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"GCD"})," uses a dedicated open zone, so that garbage-collected data is not mixed\nwith file data being written. Using a dedicated open zone for garbage collection\nreduces write amplification as garbage collected data can be assumed to be\nlonger-lived than incoming user data. Adequate free space is set aside for ",(0,i.jsx)(n.em,{children:"GCD"}),"\nso that it is never blocked by user writes and can provide a free zone to user\nwriters as long as the file system reports free blocks."]}),"\n",(0,i.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,i.jsx)(n.p,{children:"Because zoned devices do not allow in-place updates, persistent preallocations\ncan fundamentally not be supported with a zoned block device."}),"\n",(0,i.jsx)(n.p,{children:"Additionally, disk quotas and reference-counted block sharing (reflink) are\ncurrently not supported (that is expected to change in the near future)."}),"\n",(0,i.jsx)(n.h2,{id:"system-requirements",children:"System Requirements"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"XFS"})," native zoned block device support was introduced with kernel version 6.15.\nThe kernel must be compiled with ",(0,i.jsx)(n.em,{children:"XFS"})," support (the kernel configuration\nparameter CONFIG_XFS_FS must be enabled). ",(0,i.jsx)(n.em,{children:"XFS"})," realtime device feature is also\nrequired (the kernel configuration parameter CONFIG_XFS_RT must be enabled)."]}),"\n",(0,i.jsxs)(n.p,{children:["Installing version 6.15 or higher of the ",(0,i.jsx)(n.em,{children:"xfsprogs"})," package providing ",(0,i.jsx)(n.em,{children:"XFS"}),"\nuser space tools (e.g. ",(0,i.jsx)(n.code,{children:"mkfs.xfs"}),") is also required."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-plaintext",children:"# mkfs.xfs -V\nmkfs.xfs version 6.15.0\n"})}),"\n",(0,i.jsxs)(n.p,{children:["If your Linux distribution does not provides an adequate version of ",(0,i.jsx)(n.em,{children:"xfsprogs"}),",\nthis project source code can be downloaded from ",(0,i.jsx)("a",{href:"https://git.kernel.org/pub/scm/fs/xfs/xfsprogs-dev.git",target:"_blank",children:"\nkernel.org"}),". See the ",(0,i.jsx)("a",{href:"https://git.kernel.org/pub/scm/fs/xfs/xfsprogs-dev.git/tree/doc/INSTALL",target:"_blank",children:"INSTALL"})," file for instructions on how to compile and install\nthis package."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);